{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader('Pranav Sai Putta Resume.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pranav Sai Putta  \\nArlington, VA  •  202 -391 -3651  •  Putta.Pranavsai98@gmail.com    \\nhttps://www.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits. \n",
    "\n",
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pranav Sai Putta  \\nArlington, VA  •  202 -391 -3651  •  Putta.Pranavsai98@gmail.com    \\nhttps://www.linkedin.com/in/pranavsai/   •  https://github.com/pranavsai -98  •  https://www.pranavsaiputta.com/  \\nPROFESSIONAL SUMMARY  \\nExperienced Data Scientist with proficiency in statistical analysis, machine learning, and data -driven decision -making. \\nCollaborat ed with cross -functional teams to understand business objectives. Skilled  at optimizing data models  and big data'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The stand-out features include the medical coding automation, the anime recommendation engine, the predictive solution to automate medical coding from detailed physician reports, the \"Anime Whisperer\" feature, and the analytic dashboard.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"stand out features\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
      "                                              0.0/8.4 MB ? eta -:--:--\n",
      "                                              0.1/8.4 MB 6.4 MB/s eta 0:00:02\n",
      "     --                                       0.5/8.4 MB 8.1 MB/s eta 0:00:01\n",
      "     -------                                  1.6/8.4 MB 14.5 MB/s eta 0:00:01\n",
      "     ---------------                          3.2/8.4 MB 22.9 MB/s eta 0:00:01\n",
      "     -------------------------                5.4/8.4 MB 31.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      7.4/8.4 MB 36.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.4/8.4 MB 33.7 MB/s eta 0:00:00\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.1.2-py3-none-any.whl (516 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit)\n",
      "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (8.1.7)\n",
      "Collecting importlib-metadata<7,>=1.4 (from streamlit)\n",
      "  Using cached importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (1.26.2)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (23.2)\n",
      "Collecting pandas<3,>=1.3.0 (from streamlit)\n",
      "  Using cached pandas-2.1.3-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (10.1.0)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-4.25.1-cp310-abi3-win_amd64.whl (413 kB)\n",
      "                                              0.0/413.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 413.4/413.4 kB ? eta 0:00:00\n",
      "Collecting pyarrow>=6.0 (from streamlit)\n",
      "  Using cached pyarrow-14.0.1-cp311-cp311-win_amd64.whl (24.6 MB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (8.2.3)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from streamlit) (4.8.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Using cached validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from streamlit) (6.3.3)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-3.0.0-py3-none-win_amd64.whl (82 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<7,>=1.4->streamlit)\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.3.0->streamlit)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas<3,>=1.3.0->streamlit)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.27->streamlit) (2023.11.17)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pc\\appdata\\roaming\\python\\python311\\site-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\pc\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema_specifications-2023.11.1-py3-none-any.whl (17 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.31.0-py3-none-any.whl (25 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.13.1-cp311-none-win_amd64.whl (188 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, zipp, watchdog, validators, tzdata, toolz, toml, smmap, rpds-py, pyarrow, protobuf, mdurl, cachetools, blinker, tzlocal, referencing, pydeck, pandas, markdown-it-py, importlib-metadata, gitdb, rich, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "Successfully installed altair-5.1.2 blinker-1.7.0 cachetools-5.3.2 gitdb-4.0.11 gitpython-3.1.40 importlib-metadata-6.8.0 jsonschema-4.20.0 jsonschema-specifications-2023.11.1 markdown-it-py-3.0.0 mdurl-0.1.2 pandas-2.1.3 protobuf-4.25.1 pyarrow-14.0.1 pydeck-0.8.1b0 pytz-2023.3.post1 referencing-0.31.0 rich-13.7.0 rpds-py-0.13.1 smmap-5.0.1 streamlit-1.28.2 toml-0.10.2 toolz-0.12.0 tzdata-2023.3 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0 zipp-3.17.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script watchmedo.exe is installed in 'c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown-it.exe is installed in 'c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script jsonschema.exe is installed in 'c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script streamlit.exe is installed in 'c:\\Users\\PC\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This document highlights the education and data science projects of a student who has a Master of Science in Data Science from George Washington University. The projects cover using machine learning to reduce coding time and claim rejections, creating an anime recommendation engine, and developing a predictive solution to automate medical coding.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "import os\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "\n",
    "pdfReader = PdfReader('Pranav Sai Putta Resume.pdf')\n",
    "raw_text = ''\n",
    "for page in pdfReader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)\n",
    "embeddings = OpenAIEmbeddings() \n",
    "docsearch = FAISS.from_texts(texts, embeddings)\n",
    "\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")\n",
    "query = \"summary of document\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x22e2872b550>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
